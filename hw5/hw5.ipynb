{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import csv\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.mlab as mlab\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy as sp\n",
    "import scipy.io as sio\n",
    "# import scipy.stats as stats\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPAM = 'dist/spam_data.mat'\n",
    "TRAIN_CENSUS = 'hw5_census_dist/train_data.csv'\n",
    "TRAIN_TITANIC = 'hw5_titanic_dist/titanic_training.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_data = sio.loadmat(SPAM)\n",
    "census_tdata = pd.read_csv(TRAIN_CENSUS)\n",
    "titanic_tdata = pd.read_csv(TRAIN_TITANIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Removed ticket and cabin features\"\"\"\n",
    "titanic_tdata_filtered = titanic_tdata.drop('ticket', 1)\n",
    "titanic_tdata_filtered = titanic_tdata_filtered.drop('cabin', 1)\n",
    "\n",
    "\"\"\"Row 707 is empty in titanic_training.csv\"\"\"\n",
    "titanic_tdata_filtered = titanic_tdata_filtered.dropna(how='all')\n",
    "\n",
    "\"\"\"Map categories to binary variables [1(a)]\"\"\"\n",
    "tv = DictVectorizer()\n",
    "vectorized_titanic = tv.fit_transform(titanic_tdata_filtered.to_dict(orient='records'))\n",
    "\n",
    "\"\"\"Replaced unknown values with mean of features [2(b)]\"\"\"\n",
    "timp = Imputer()\n",
    "imputed_titanic = timp.fit_transform(vectorized_titanic)\n",
    "\n",
    "ttdata = pd.DataFrame(data=imputed_titanic.toarray(), columns=tv.get_feature_names(), dtype='object')\n",
    "# ttlabels = ttdata['survived']\n",
    "# ttdata = ttdata.drop('survived', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Map categories to binary variables [1(a)]\"\"\"\n",
    "cv = DictVectorizer()\n",
    "vectorized_census = cv.fit_transform(census_tdata.to_dict(orient='records'))\n",
    "\n",
    "\"\"\"Replaced unknown values with mean of features [2(b)]\"\"\"\n",
    "cimp = Imputer()\n",
    "imputed_census = cimp.fit_transform(vectorized_census)\n",
    "\n",
    "ctdata = pd.DataFrame(data=imputed_census.toarray(), columns=cv.get_feature_names(), dtype='object')\n",
    "# ctlabels = ctdata['label']\n",
    "# ctdata = ctdata.drop('label', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdata = spam_data['training_data']\n",
    "stlabels = spam_data['training_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Abstraction\n",
    "## Class InternalNode\n",
    "* ### State\n",
    "    * Node left, right\n",
    "    * split_feature [split rule]\n",
    "    * split_value [split rule]\n",
    "* ### Methods\n",
    "    * `predict(data)`\n",
    "        * given a data point, chooses left or right child based on the split rule\n",
    "        * traverses starting from this node\n",
    "    * `is_leaf() { return False }`\n",
    "\n",
    "## Class LeafNode\n",
    "* ### State\n",
    "    * label\n",
    "* ### Methods\n",
    "    * `is_leaf() { return True }`\n",
    "\n",
    "## Class DecisionTree\n",
    "* ### State\n",
    "    * root\n",
    "* ### Methods\n",
    "    * `impurity(left_label_hist, right_label_hist)`\n",
    "        * calculates the entropy of a split\n",
    "    * `segmenter(data, labels)`\n",
    "        * finds the best split rule using impurity()\n",
    "        * many different types of segmenters\n",
    "    * `train(train_data, train_labels, depth_limited)`\n",
    "        * grows the decision tree\n",
    "        * uses segmenter to find the best splits\n",
    "    * `predict(data)`\n",
    "        * given a data point, traverses the tree starting at the root\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeafNode:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self.label\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return True\n",
    "    \n",
    "class InternalNode:\n",
    "    def __init__(self, left, right, split_feature, split_value):\n",
    "        self.left, self.right = left, right\n",
    "        self.split_feature = split_feature \n",
    "        self.split_value = split_value\n",
    "    \n",
    "    def predict(self, data):\n",
    "        if data[self.split_feature] < self.split_value:\n",
    "            return self.left.predict(data)\n",
    "        return self.right.predict(data)\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return False\n",
    "        \n",
    "class PandasDecisionTree:\n",
    "    def __init__(self, root=None):\n",
    "        self.root = root\n",
    "        \n",
    "    def logsp(self, x, y):\n",
    "        if x == 0:\n",
    "            return 0\n",
    "        return x * log2(x / (x + y))\n",
    "        \n",
    "    def impurity(self, C, D, nC, nD):\n",
    "        c, d = nC - C, nD - D\n",
    "        return -(self.logsp(C, D) + self.logsp(D, C) + self.logsp(c, d) + self.logsp(d, c)) / (nC + nD)\n",
    "        \n",
    "    def count(self, data, label):\n",
    "        nC, nD = 0, 0\n",
    "        for lbl in data[label]:\n",
    "            if lbl == 0:\n",
    "                nC += 1\n",
    "            else:\n",
    "                nD += 1\n",
    "        return nC, nD\n",
    "        \n",
    "    def segmenter(self, data, label):\n",
    "        \"\"\"\n",
    "        For splits on a feature f with a value v, \n",
    "            left will have samples with f values strictly less than v\n",
    "            right will have samples with f values greater than or equal to v \n",
    "        \"\"\"\n",
    "        nC, nD = self.count(data, label)\n",
    "        features = data.drop(label, 1).axes[1]\n",
    "        \n",
    "        min_entropy = float('inf')\n",
    "        splitf = features[0]\n",
    "#         print(len(data[splitf]))\n",
    "        splitv = next(data[splitf].__iter__())\n",
    "        spliti = max(len(data) // 2, 1)\n",
    "        \n",
    "        for f in features:\n",
    "            sorted_data_by_f = data.sort_values(f)\n",
    "            iterv = sorted_data_by_f[f].__iter__()\n",
    "            iterl = sorted_data_by_f[label].__iter__()\n",
    "            \n",
    "            # keeps track of which value-label pair we're on\n",
    "            v, lbl, i = next(iterv), next(iterl), 0\n",
    "            \n",
    "            # keeps track of class counts on LEFT\n",
    "            C, D = 0, 0\n",
    "            \n",
    "            # don't check if all data lies on one side, since then this node should be a leaf (base case of train)\n",
    "            beta_iter = sorted_data_by_f.drop_duplicates(subset=f, keep='first')[f].__iter__()\n",
    "            next(beta_iter)\n",
    "            \n",
    "            for beta in beta_iter:\n",
    "                try:\n",
    "                    while v != beta:\n",
    "                        if lbl == 0:\n",
    "                            C += 1\n",
    "                        else:\n",
    "                            D += 1\n",
    "                        i += 1\n",
    "                        v, lbl = next(iterv), next(iterl)\n",
    "                except StopIteration:\n",
    "                    continue\n",
    "                entropy = self.impurity(C, D, nC, nD)\n",
    "#                 print(f, beta, i, C, D, entropy)\n",
    "                if entropy < min_entropy:\n",
    "                    min_entropy, splitf, splitv, spliti = entropy, f, beta, i\n",
    "            # no need to check split on max(beta)+1 since then all elements are on one split (node should be leaf)\n",
    "            \n",
    "        if splitf == None:\n",
    "            raise Exception('{}\\n{}'.format(data, features))\n",
    "            \n",
    "        sorted_data = data.sort_values(splitf)\n",
    "        left = sorted_data.iloc[:spliti]\n",
    "        right = sorted_data.iloc[spliti:]\n",
    "        \n",
    "        return left, right, splitf, splitv #, spliti\n",
    "        \n",
    "    def train(self, train_data, label, depth_limited=float('inf')):\n",
    "        def grow_tree(train_data, label, depth_limited=float('inf')):\n",
    "            labels = train_data[label].unique()\n",
    "            if depth_limited == 0 or len(labels) == 1:\n",
    "                return LeafNode(labels[0])\n",
    "            left_data, right_data, split_feature, split_value = self.segmenter(train_data, label)\n",
    "            left = grow_tree(left_data, label, depth_limited - 1)\n",
    "            right = grow_tree(right_data, label, depth_limited - 1)\n",
    "            return InternalNode(left, right, split_feature, split_value)\n",
    "        self.root = grow_tree(train_data, label, depth_limited)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self.root.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8282828282828283 82 99\n"
     ]
    }
   ],
   "source": [
    "shuffled_ttdata = ttdata.sample(frac=1)\n",
    "tvnum = int(len(shuffled_ttdata) // 10)\n",
    "vsttdata = shuffled_ttdata[:tvnum]\n",
    "tsttdata = shuffled_ttdata[tvnum:]\n",
    "\n",
    "tlabel = 'survived'\n",
    "tdt = PandasDecisionTree()\n",
    "tdt.train(tsttdata, tlabel)\n",
    "\n",
    "correct = 0\n",
    "for _, vsample in vsttdata.iterrows():\n",
    "    if tdt.predict(vsample) == vsample[tlabel]:\n",
    "        correct += 1\n",
    "print(correct / len(vsttdata), correct, len(vsttdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled_ctdata = ctdata.sample(frac=1)\n",
    "cvnum = int(len(shuffled_ctdata) // 10)\n",
    "vsctdata = shuffled_ctdata[:cvnum]\n",
    "tsctdata = shuffled_ctdata[cvnum:]\n",
    "\n",
    "clabel = 'label'\n",
    "cdt = PandasDecisionTree()\n",
    "cdt.train(tsctdata, clabel)\n",
    "\n",
    "correct = 0\n",
    "for _, vsample in vsctdata.iterrows():\n",
    "    if cdt.predict(vsample) == vsample[clabel]:\n",
    "        correct += 1\n",
    "print(correct / len(vsctdata), correct, len(vsctdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_titanic = ttdata[:30]\n",
    "small_tdt = PandasDecisionTree()\n",
    "# small_titanic['survived'].unique()\n",
    "small_tdt.train(small_titanic, 'survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "p = small_tdt.predict(ttdata.iloc[345])\n",
    "print(p, ttdata['survived'][345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# q = []\n",
    "# q.append(small_tdt.root)\n",
    "# while len(q) > 0:\n",
    "#     n = q.pop(0)\n",
    "#     if not n.is_leaf():\n",
    "#         print(n.split_feature, n.split_value, '\\n')\n",
    "#         q.append(n.left)\n",
    "#         q.append(n.right)\n",
    "#     else:\n",
    "#         print('label: ', n.label, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-03939608a710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTitanicDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspliti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "k, a, b = 'survived', 'apple', 'banana'\n",
    "d = [{k: 0, a: 1, b: 1},\n",
    "     {k: 1, a: 1, b: 77},\n",
    "     {k: 1, a: 1, b: 4},\n",
    "     {k: 0, a: 1, b: 1}, \n",
    "     {k: 0, a: 1, b: 2}]\n",
    "\n",
    "# d = [{k: 0, a: 0, b: 1},\n",
    "#      {k: 1, a: 0, b: 0}]\n",
    "\n",
    "dfd = pd.DataFrame(d)\n",
    "tdt = TitanicDecisionTree()\n",
    "left, right, splitf, splitv, spliti = tdt.segmenter(dfd, k)\n",
    "print('\\n', left, '\\n')\n",
    "print(right, '\\n')\n",
    "print(splitf, splitv, spliti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "d = [{k: 7, a: 0, b: 1}]\n",
    "\n",
    "dfd = pd.DataFrame(d)\n",
    "print(dfd[k][0])\n",
    "\n",
    "print(ttdata['survived'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "999\n",
      "999\n",
      "age           30.2594\n",
      "embarked            0\n",
      "embarked=C          0\n",
      "embarked=Q          0\n",
      "embarked=S          1\n",
      "fare             8.05\n",
      "parch               0\n",
      "pclass              3\n",
      "sex=female          0\n",
      "sex=male            1\n",
      "sibsp               0\n",
      "survived            0\n",
      "Name: 705, dtype: object\n",
      "{0.0, 1.0} \n",
      "\n",
      "age\n",
      "age           80\n",
      "embarked       0\n",
      "embarked=C     0\n",
      "embarked=Q     0\n",
      "embarked=S     1\n",
      "fare          30\n",
      "parch          0\n",
      "pclass         1\n",
      "sex=female     0\n",
      "sex=male       1\n",
      "sibsp          0\n",
      "survived       1\n",
      "Name: 475, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(ttdata.iloc[0])\n",
    "print(ttdata['survived'][0])\n",
    "print(len(ttdata))\n",
    "print(ttdata.shape[0])\n",
    "print(ttdata.iloc[705])\n",
    "print(set(ttdata['survived']), '\\n')\n",
    "s = set(ttdata.axes[1])\n",
    "# for f in ttdata.drop('survived', 1).axes[1]:\n",
    "#     print(f, type(f))\n",
    "    \n",
    "features = ttdata.drop('survived', 1).axes[1]\n",
    "for f in features:\n",
    "    print(f)\n",
    "    sorted_data_by_f = ttdata.sort_values(f)\n",
    "    print(sorted_data_by_f.iloc[-1])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name last_name  age  preTestScore  postTestScore\n",
      "0      Jason    Miller   42             4             25\n",
      "1      Jason    Miller   42             4             25\n",
      "2       Tina       Ali   36            31             57\n",
      "3       Jake    Milner   24             2             62\n",
      "4        Amy     Cooze   73             3             70\n",
      "5       Zach    Miller    4           100            100\n",
      "  first_name last_name  age  preTestScore  postTestScore\n",
      "0      Jason    Miller   42             4             25\n",
      "2       Tina       Ali   36            31             57\n",
      "3       Jake    Milner   24             2             62\n",
      "4        Amy     Cooze   73             3             70\n",
      "5       Zach    Miller    4           100            100\n"
     ]
    }
   ],
   "source": [
    "raw_data = {'first_name': ['Jason', 'Jason', 'Tina', 'Jake', 'Amy', 'Zach'],\n",
    "        'last_name': ['Miller', 'Miller', 'Ali', 'Milner', 'Cooze', 'Miller'],\n",
    "        'age': [42, 42, 36, 24, 73, 4],\n",
    "        'preTestScore': [4, 4, 31, 2, 3, 100],\n",
    "        'postTestScore': [25, 25, 57, 62, 70, 100]}\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore'])\n",
    "print(df)\n",
    "print(df.drop_duplicates(subset='first_name', keep='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
