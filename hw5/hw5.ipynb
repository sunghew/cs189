{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import csv\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.mlab as mlab\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy as sp\n",
    "import scipy.io as sio\n",
    "# import scipy.stats as stats\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPAM = 'dist/spam_data.mat'\n",
    "TRAIN_CENSUS = 'hw5_census_dist/train_data.csv'\n",
    "TRAIN_TITANIC = 'hw5_titanic_dist/titanic_training.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_data = sio.loadmat(SPAM)\n",
    "census_tdata = pd.read_csv(TRAIN_CENSUS)\n",
    "titanic_tdata = pd.read_csv(TRAIN_TITANIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Removed ticket and cabin features\"\"\"\n",
    "titanic_tdata_filtered = titanic_tdata.drop('ticket', 1)\n",
    "titanic_tdata_filtered = titanic_tdata_filtered.drop('cabin', 1)\n",
    "\n",
    "\"\"\"Row 707 is empty in titanic_training.csv\"\"\"\n",
    "titanic_tdata_filtered = titanic_tdata_filtered.dropna(how='all')\n",
    "\n",
    "\"\"\"Map categories to binary variables [1(a)]\"\"\"\n",
    "tv = DictVectorizer()\n",
    "vectorized_titanic = tv.fit_transform(titanic_tdata_filtered.to_dict(orient='records'))\n",
    "\n",
    "\"\"\"Replaced unknown values with mean of features [2(b)]\"\"\"\n",
    "timp = Imputer()\n",
    "imputed_titanic = timp.fit_transform(vectorized_titanic)\n",
    "\n",
    "ttdata = pd.DataFrame(data=imputed_titanic.toarray(), columns=tv.get_feature_names(), dtype='object')\n",
    "# ttlabels = ttdata['survived']\n",
    "# ttdata = ttdata.drop('survived', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Map categories to binary variables [1(a)]\"\"\"\n",
    "cv = DictVectorizer()\n",
    "vectorized_census = cv.fit_transform(census_tdata.to_dict(orient='records'))\n",
    "\n",
    "\"\"\"Replaced unknown values with mean of features [2(b)]\"\"\"\n",
    "cimp = Imputer()\n",
    "imputed_census = cimp.fit_transform(vectorized_census)\n",
    "\n",
    "ctdata = pd.DataFrame(data=imputed_census.toarray(), columns=cv.get_feature_names(), dtype='object')\n",
    "# ctlabels = ctdata['label']\n",
    "# ctdata = ctdata.drop('label', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdata = spam_data['training_data']\n",
    "stlabels = spam_data['training_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Abstraction\n",
    "## Class InternalNode\n",
    "* ### State\n",
    "    * Node left, right\n",
    "    * split_feature [split rule]\n",
    "    * split_value [split rule]\n",
    "* ### Methods\n",
    "    * `predict(data)`\n",
    "        * given a data point, chooses left or right child based on the split rule\n",
    "        * traverses starting from this node\n",
    "    * `is_leaf() { return False }`\n",
    "\n",
    "## Class LeafNode\n",
    "* ### State\n",
    "    * label\n",
    "* ### Methods\n",
    "    * `is_leaf() { return True }`\n",
    "\n",
    "## Class DecisionTree\n",
    "* ### State\n",
    "    * root\n",
    "* ### Methods\n",
    "    * `impurity(left_label_hist, right_label_hist)`\n",
    "        * calculates the entropy of a split\n",
    "    * `segmenter(data, labels)`\n",
    "        * finds the best split rule using impurity()\n",
    "        * many different types of segmenters\n",
    "    * `train(train_data, train_labels, depth_limited)`\n",
    "        * grows the decision tree\n",
    "        * uses segmenter to find the best splits\n",
    "    * `predict(data)`\n",
    "        * given a data point, traverses the tree starting at the root\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-209-1d5df68218ae>, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-209-1d5df68218ae>\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    left =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LeafNode:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        \n",
    "    def predict(data):\n",
    "        return self.label\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return True\n",
    "    \n",
    "class InternalNode:\n",
    "    def __init__(self, left, right, split_feature, split_value):\n",
    "        self.left, self.right = left, right\n",
    "        self.split_feature = split_feature \n",
    "        self.split_value = split_value\n",
    "    \n",
    "    def predict(data):\n",
    "        \"\"\"IMPLEMENT ME\"\"\"\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return False\n",
    "        \n",
    "class TitanicDecisionTree:\n",
    "    def __init__(self, root=None):\n",
    "        self.root = root\n",
    "        \n",
    "    def impurity(self, C, D, nC, nD):\n",
    "        c, d = nC - C, nD - D\n",
    "        lefte  = C * log2(C / (C + D)) / sumCD + D * log2(D / (C + D))\n",
    "        righte = c * log2(c / (c + d)) / sumcd + d * log2(d / (c + d))\n",
    "        return -(lefte + righte) / (nC + nD)\n",
    "        \n",
    "    def count(self, data, label):\n",
    "        nC, nD = 0, 0\n",
    "        for lbl in data[label]:\n",
    "            if lbl == 0:\n",
    "                nC += 1\n",
    "            else:\n",
    "                nD += 1\n",
    "        return nC, nD\n",
    "        \n",
    "    def segmenter(self, data, label):\n",
    "        \"\"\"\n",
    "        For splits on a feature f with a value v, \n",
    "            left will have samples with f values strictly less than v\n",
    "            right will have samples with f values greater than or equal to v \n",
    "        \"\"\"\n",
    "        nC, nD = self.count(data)\n",
    "        min_entropy, splitf, splitv = float('inf'), None, None\n",
    "        features = data.drop(label, 1).axes[1]\n",
    "        for f in features:\n",
    "            sorted_data_by_f = data.sort_values(f, axis=1)\n",
    "            i, C, D = 0, 0, 0\n",
    "            for beta in sorted_data_by_f.drop_duplicates(subset=f, keep='first'):\n",
    "                while sorted_data_by_f[f][i] != beta:\n",
    "                    if data[label][i] == 0:\n",
    "                        C += 1\n",
    "                    else:\n",
    "                        D += 1\n",
    "                    i += 1\n",
    "                entropy = self.impurity(C, D, nC, nD)\n",
    "                if entropy < min_entropy:\n",
    "                    min_entropy, splitf, splitv = entropy, f, beta\n",
    "            # no need to check split on max(beta)+1 since it's the same as min(beta)\n",
    "            # i.e all elements are on one split\n",
    "                    \n",
    "        left = \n",
    "        right = \n",
    "        return left, right, splitf, splitv\n",
    "                    \n",
    "                    \n",
    "        \n",
    "    def train(self, train_data, label, depth_limited=float('inf')):\n",
    "        labels = set(train_data[label])\n",
    "        if depth_limited == 0 or len(labels) == 1:\n",
    "            return LeafNode(labels.pop())\n",
    "        left_data, right_data, split_feature, split_value = self.segmenter(train_data, label)\n",
    "        left = self.train(left_data, depth_limited - 1)\n",
    "        right = self.train(right_data, depth_limited - 1)\n",
    "        return new InternalNode(left, right, split_feature, split_value)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self.root.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "999\n",
      "999\n",
      "age           30.2594\n",
      "embarked            0\n",
      "embarked=C          0\n",
      "embarked=Q          0\n",
      "embarked=S          1\n",
      "fare             8.05\n",
      "parch               0\n",
      "pclass              3\n",
      "sex=female          0\n",
      "sex=male            1\n",
      "sibsp               0\n",
      "survived            0\n",
      "Name: 705, dtype: object\n",
      "{0.0, 1.0} \n",
      "\n",
      "age\n",
      "age           80\n",
      "embarked       0\n",
      "embarked=C     0\n",
      "embarked=Q     0\n",
      "embarked=S     1\n",
      "fare          30\n",
      "parch          0\n",
      "pclass         1\n",
      "sex=female     0\n",
      "sex=male       1\n",
      "sibsp          0\n",
      "survived       1\n",
      "Name: 475, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(ttdata.iloc[0])\n",
    "print(ttdata['survived'][0])\n",
    "print(len(ttdata))\n",
    "print(ttdata.shape[0])\n",
    "print(ttdata.iloc[705])\n",
    "print(set(ttdata['survived']), '\\n')\n",
    "s = set(ttdata.axes[1])\n",
    "# for f in ttdata.drop('survived', 1).axes[1]:\n",
    "#     print(f, type(f))\n",
    "    \n",
    "features = ttdata.drop('survived', 1).axes[1]\n",
    "for f in features:\n",
    "    print(f)\n",
    "    sorted_data_by_f = ttdata.sort_values(f)\n",
    "    print(sorted_data_by_f.iloc[-1])\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name last_name  age  preTestScore  postTestScore\n",
      "0      Jason    Miller   42             4             25\n",
      "1      Jason    Miller   42             4             25\n",
      "2       Tina       Ali   36            31             57\n",
      "3       Jake    Milner   24             2             62\n",
      "4        Amy     Cooze   73             3             70\n",
      "5       Zach    Miller    4           100            100\n",
      "  first_name last_name  age  preTestScore  postTestScore\n",
      "0      Jason    Miller   42             4             25\n",
      "2       Tina       Ali   36            31             57\n",
      "3       Jake    Milner   24             2             62\n",
      "4        Amy     Cooze   73             3             70\n",
      "5       Zach    Miller    4           100            100\n"
     ]
    }
   ],
   "source": [
    "raw_data = {'first_name': ['Jason', 'Jason', 'Tina', 'Jake', 'Amy', 'Zach'],\n",
    "        'last_name': ['Miller', 'Miller', 'Ali', 'Milner', 'Cooze', 'Miller'],\n",
    "        'age': [42, 42, 36, 24, 73, 4],\n",
    "        'preTestScore': [4, 4, 31, 2, 3, 100],\n",
    "        'postTestScore': [25, 25, 57, 62, 70, 100]}\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore'])\n",
    "print(df)\n",
    "print(df.drop_duplicates(subset='first_name', keep='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
